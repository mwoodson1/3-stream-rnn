{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning\n",
    "My hope is that this will be a good introduction to what deep learning is, what the objective of it is, how to use it, when to use it, and many of its derivative algorithms.\n",
    "\n",
    "## Perceptron\n",
    "The basic unit of a neural network is the perceptron displayed below. It takes in a variable amount of inputs, aggregates them according to sum function (could be a linear sum or something else entirely) and then produces an output depending on the output.\n",
    "![Perceptron](img191.gif)\n",
    "\n",
    "So you have an input vector $X$, a set of weights $W$ that place a weight on each $x_i \\in X$, an aggregation function $f$, and a activation function $f_i$ which determines if the perceptron will output a 1 or 0(in the binary case). \n",
    "\n",
    "The learning algorithm for a perceptron is rather simple and you can read a complete description [here](https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm). The idea is:\n",
    "\n",
    "1. Randomly initialize the weights.\n",
    "2. For 1 training example calculate $y_i = f_i (f(X*W))$.\n",
    "3. $w_inew = w_i + \\alpha * t_j-y_i$ (update the weight with the difference between the target and the actual output scaled by some learning rate alpha.\n",
    "\n",
    "## Multi-layer Perceptrons (Neural Networks)\n",
    "The basic kind of neural network is the multi-layer perceptron(MLP). Simply stated it is just a network of layers of perceptrons. An example is shown below.\n",
    "![Multi-layer Perceptron](MLFNwithWeights.jpg)\n",
    "\n",
    "Now we have a matrix $Wi$ between each layer of perceptrons with $w_{ij} \\in W$ being the weight between node $i$ in the pervious layer to node $j$ in the next. Each node still has an activation function but now we can have more non-linearity introduced as the output is now a non-linear combination of a non-linear combination of the inputs.\n",
    "\n",
    "## Backpropogation\n",
    "The training procedure for a multi-layey proceptron is considerably more complex. The process is called backpropogation. The procedure is explained EXCELENTLY [here](http://iamtrask.github.io/2015/07/12/basic-python-network/) and I highly reccomend going through the exercise of implementing it yourself. \n",
    "\n",
    "The basic idea is to perform forward propogation which is to simply run a training test case through the network and get an output. If it is incorrect then calculate the the error with respect to a particular node. This is done byu going backwards through the layers and calculating the partial derivative of the error with respect to the output of the nonlinear activation function $f_i$ of the node in consideration. This error then gets propogated to the previous layers nodes who connected to it. Below will be some example code that is in the link above but I will attempt to summarize the work quickly with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fa7b6ceab10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEZCAYAAABxbJkKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7b6cf6c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np   #<--- a really good matrix / linear algebra library\n",
    "import matplotlib    #plotting library similar to matlabs plotting\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "'''\n",
    "This will be our activation function which is a sigmoid(plotted below). \n",
    "When doing backpropogation we will need to take the derivative of this\n",
    "function so we take in a input to make it easier to do so\n",
    "'''\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x = np.linspace(-5.0,5.0,400)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title(\"Sigmoid Activation Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid activation function is the simplest type of activation function but since it's inception there have been many many more types of activation functions that include \n",
    "- Tanh\n",
    "- Linear\n",
    "- Step\n",
    "- Rectified linear\n",
    "\n",
    "Using a different function is not hard as long as you can provide the derivative of that function as well. Nothing complicated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    " \n",
    "# output dataset           \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "#It's always good practice to seed your random number generator\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "Here we are just goiing to initialize the weights of our 3 layer network randomly with mean 0. Note that this is not the only way to initialize the weights. If you know something about the data you are working with there might be a better way to initialize them. Other initialization processes also do exist but the purpose of them all is the same, set all the weights to something either completely random or closer to what they should be after convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W0 = 2*np.random.random((3,4)) - 1\n",
    "W1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propogation\n",
    "This is the simple part of the training process. Take your data and run it through the network to get an output. This is as simple as multiplying matrices. Write out the matrix math and confirm that this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l0 = X                       #first layer is input data\n",
    "l1 = sigmoid(np.dot(l0,W0))   #multiply inputs by the weights to the first layer\n",
    "l2 = sigmoid(np.dot(l1,W1))   #multiply the outputs from the previous layer by the\n",
    "                             #weights of the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Calculation\n",
    "This is another step that has many solutions. We need to calculate the error we have made with the network we have. We could use l1,l2,l3, entropy, or cross-categorical entropy, or any other type of error function you can define. For simplicities sake we will stick with raw difference as our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_error = y - l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation\n",
    "Once we have the error we can start backpropogation. Thhe first step here is to take the derivative of our error with respect to the second layers weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l2_delta = l2_error*sigmoid(l2,deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we propogate this error to the first layer to see how much the first layer contributed to the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_error = l2_delta.dot(W1.T)\n",
    "l1_delta = l1_error * sigmoid(l1,deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can update the weights in many ways as well. The way chosen here is the simplest but by far not the best way to do things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 += l1.T.dot(l2_delta)\n",
    "W0 += l0.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it together\n",
    "We just went through 1 iteration but that is almost always not enough. We must repeat this process until a convergence criteria is met. This will minimize your error. A for loop over a lot of iterations is simple enough to do but again, better methods exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.464193701547\n",
      "Error:0.00500215791205\n",
      "Error:0.00345431369834\n",
      "Error:0.00278650792965\n",
      "Error:0.00239408397566\n",
      "Error:0.00212886289005\n"
     ]
    }
   ],
   "source": [
    "for j in xrange(60000):\n",
    "    l0 = X                       #first layer is input data\n",
    "    l1 = sigmoid(np.dot(l0,W0))   #multiply inputs by the weights to the first layer\n",
    "    l2 = sigmoid(np.dot(l1,W1))   #multiply the outputs from the previous layer by the\n",
    "                                 #weights of the last layer\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if(j%10000) == 0:\n",
    "        print \"Error:\" + str(np.mean(np.abs(l2_error)))\n",
    "    \n",
    "    l2_delta = l2_error*sigmoid(l2,deriv=True)\n",
    "    \n",
    "    l1_error = l2_delta.dot(W1.T)\n",
    "    l1_delta = l1_error * sigmoid(l1,deriv=True)\n",
    "    \n",
    "    W1 += l1.T.dot(l2_delta)\n",
    "    W0 += l0.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
