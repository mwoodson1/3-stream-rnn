{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy and Keras Tutorial\n",
    "This will just be a quick intoduction to numpy, matplotlib, and how to make your own architecture in Keras. First since you guys asked I will go over why the math of backpropogation is the way it is.\n",
    "\n",
    "## Backpropogation\n",
    "The general idea of backpropogation is that we cannot determine what a hidden unit should do but we can calculate how fast the error changes as we change hidden activity. So we are using error derivatives with respect to hidden activities. But remember that each hidden activity has many incoming and outgoing connections which we must take into account when doing the math.\n",
    "\n",
    "First let us define the error as such\n",
    "$$ E = \\frac{1}{2} \\sum_{j \\in output}(t_j-y_j)^2$$\n",
    "\n",
    "Then we know\n",
    "$$\\frac{\\partial E}{\\partial y_j} = -(t_j-y_j)$$\n",
    "\n",
    "This gives us an error derivative for when we change the activity of an output unit $y_j$. Also just note that $t_j$ is our target class in this case, just a change in notation from earlier today. So now that we have computed these error derivatives for the output layer we want to somehow use this to solve for the error derivatives in the layer before it. \n",
    "\n",
    "For notations sake assume layer $i$ comes BEFORE layer $j$ in the feed forward process, let $y_i$ be the output of some hidden unit $i$ in layer $i$ and $y_j$ be the output of node $j$ in layer $j$. Then define the sum of all the activities inputs into unit $j$ as $z_j$. Then we can calculate\n",
    "$$\\frac{\\partial E}{\\partial z_j} = \\frac{dy_j}{dz_j}\\frac{\\partial E}{\\partial y_j} = y_j(1-y_j)\\frac{\\partial E}{\\partial y_j}$$\n",
    "\n",
    "The above is just using the chain rule of calculus. Now we can compute the error derivative with respect to the output of unit $i$ and not the entire sum of connections.\n",
    "$$\\frac{\\partial E}{\\partial y_i} = \\sum_{j}\\frac{dz_j}{dy_i}\\frac{\\partial E}{\\partial z_j} = \\sum_{j}w_{ij}\\frac{\\partial E}{\\partial z_j}$$\n",
    "\n",
    "The above is deriving the first part of the equation I showed you guys in class. We simply multiply all the errors in the layer ahead of us by the connections we have to those units because that is how much we \"contributed\" to that error. But that is not all we want, we want to find out how much error our weights contributed as well. We can calculate that easily too now!\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial z_j}{\\partial w_{ij}}\\frac{\\partial E}{\\partial z_j} = y_j\\frac{\\partial E}{\\partial z_j}$$\n",
    "\n",
    "Now to see how this relates to what I showed earlier in class take the derivative of our sigmoid(or logistic) function and you should get \n",
    "$$\\frac{\\partial y}{\\partial z}(\\frac{1}{1+e^{-z}}) = y(1-y)$$\n",
    "\n",
    "You can see where $y(1-y)$ pops up in the equation above and can substitute that for g'(z) and see that you will get the exact same equation I gave you guys in class. Yes there is a sum but as you can tell any sum can be turned into a matrix multiply anyways so I won't bother with that math. If you have any confusion feel free to message me or check out the links I will put at the bottom of this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[Andrew Ng Coursera Machine Learning Course](https://www.youtube.com/watch?v=mcnvIWDnPns&index=44&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW) - The most relavent is the machine learning W4 lectures. This will go over neural networks in general but not much of the math. Also more emphasis on matrix computation than math.\n",
    "\n",
    "[Geoffrey Hinton Coursera Machine Learning Course](https://class.coursera.org/neuralnets-2012-001/lecture) - Lecture 3 is the most relavent for backpropogation but I HIGHLY HIGHLY HIGHLY HIGHLY reccomend watching more of them.....HIGHLY. These were made by the founder and god of neural networks so you can't find a better source than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
